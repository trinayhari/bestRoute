{'timestamp': '2025-05-01T13:36:30.627681', 'prompt_type': 'summary', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 28, 'completion_tokens': 482, 'total_tokens': 510, 'latency': 3.8983428478240967, 'duration': 3.898844, 'cost': 0.0001275}
{'timestamp': '2025-05-01T13:36:57.993623', 'prompt_type': 'question', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 21, 'completion_tokens': 14, 'total_tokens': 35, 'latency': 1.0944468975067139, 'duration': 1.094897, 'cost': 8.750000000000001e-06}
{'timestamp': '2025-05-01T13:37:23.344802', 'prompt_type': 'question', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 25, 'completion_tokens': 483, 'total_tokens': 508, 'latency': 4.601731061935425, 'duration': 4.602432, 'cost': 0.000127}
{'timestamp': '2025-05-01T13:51:46.437622', 'prompt_type': 'question', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 20, 'completion_tokens': 19, 'total_tokens': 39, 'latency': 0.9218099117279053, 'duration': 0.922916, 'cost': 9.75e-06}
{'timestamp': '2025-05-01T13:52:32.911977', 'prompt_type': 'question', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 45, 'completion_tokens': 525, 'total_tokens': 570, 'latency': 4.319800138473511, 'duration': 4.320703, 'cost': 0.00014250000000000002}
{'timestamp': '2025-05-01T13:53:41.837650', 'prompt_type': 'question', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 608, 'completion_tokens': 1146, 'total_tokens': 1754, 'latency': 7.746888160705566, 'duration': 7.748492, 'cost': 0.0004385}
{'timestamp': '2025-05-01T13:53:57.939417', 'prompt_type': 'question', 'model': 'openai/gpt-4o', 'prompt_tokens': 1527, 'completion_tokens': 41, 'total_tokens': 1568, 'latency': 2.0630290508270264, 'duration': 2.064712, 'cost': 0.00784}
{'timestamp': '2025-05-01T13:54:35.372731', 'prompt_type': 'code', 'model': 'openai/gpt-4o', 'prompt_tokens': 1647, 'completion_tokens': 570, 'total_tokens': 2217, 'latency': 12.970293998718262, 'duration': 12.970837, 'cost': 0.011085000000000001}
{'timestamp': '2025-05-01T13:55:02.017787', 'prompt_type': 'question', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 2574, 'completion_tokens': 628, 'total_tokens': 3202, 'latency': 4.778250217437744, 'duration': 4.779389, 'cost': 0.0008005}
{'timestamp': '2025-05-01T14:37:37.650283', 'prompt_type': 'question', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 20, 'completion_tokens': 19, 'total_tokens': 39, 'latency': 0.9769389629364014, 'duration': 0.977529, 'cost': 9.75e-06}
{'timestamp': '2025-05-01T14:38:07.490422', 'prompt_type': 'question', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 53, 'completion_tokens': 643, 'total_tokens': 696, 'latency': 4.630291938781738, 'duration': 4.631931, 'cost': 0.00017400000000000003}
{'timestamp': '2025-05-02T13:09:05.614111', 'prompt_type': 'question', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 20, 'completion_tokens': 19, 'total_tokens': 39, 'latency': 0.8757238388061523, 'duration': 0.879141, 'cost': 9.75e-06}
{'timestamp': '2025-05-02T13:10:16.797006', 'prompt_type': 'question', 'model': 'mistralai/mistral-7b-instruct', 'prompt_tokens': 45, 'completion_tokens': 15, 'total_tokens': 60, 'latency': 0.7267658710479736, 'duration': 0.727819, 'cost': 1.2e-05}
{'timestamp': '2025-05-02T13:10:34.326790', 'prompt_type': 'question', 'model': 'mistralai/mistral-7b-instruct', 'prompt_tokens': 73, 'completion_tokens': 17, 'total_tokens': 90, 'latency': 1.507676124572754, 'duration': 1.508105, 'cost': 1.8e-05}
{'timestamp': '2025-05-02T13:11:09.248827', 'prompt_type': 'question', 'model': 'anthropic/claude-3-haiku', 'prompt_tokens': 21, 'completion_tokens': 14, 'total_tokens': 35, 'latency': 0.728313684463501, 'duration': 0.72957, 'cost': 8.750000000000001e-06}
{'timestamp': '2025-05-02T13:11:55.788570', 'prompt_type': 'question', 'model': 'anthropic/claude-3-opus', 'prompt_tokens': 46, 'completion_tokens': 634, 'total_tokens': 680, 'latency': 21.584211826324463, 'duration': 21.584853, 'cost': 0.010199999999999999}
